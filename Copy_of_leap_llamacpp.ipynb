{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaravAttre/LSTM-collab/blob/main/Copy_of_leap_llamacpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozdAwO7jHNwH",
        "outputId": "e59e3824-1545-4aa7-b332-ac6d0a905530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMFWgNeRHkQd",
        "outputId": "ad13db2b-31f4-427d-9e5b-bc788c264b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422301 sha256=24c2dc3b3884edc5e056b25d2f90bf4eac693e7513cc4b3416597d6d46017249\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzBKvh_Isc2p",
        "outputId": "aa3581a2-9058-464b-bf5a-74ce92bf1646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn\n",
        "!pip install nest_asyncio\n",
        "!pip install pyngrok\n",
        "\n",
        "from fastapi import FastAPI, Query\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqT4V-b6JZci",
        "outputId": "ee790cf9-eba7-43a2-c8cd-2578db4481d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-22 16:40:36--  https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.167.112.25, 3.167.112.38, 3.167.112.96, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.167.112.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/68724ea87a4e585e699dd7fa/b7ffa1e6fd76669d8cb1fdfaaadceae255e67d3f3483c6c4d937a394bafc442d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250822%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250822T164037Z&X-Amz-Expires=3600&X-Amz-Signature=52326995326554ed45eb440039f1b0da57cc266192a1408a9534f69e47e5a008&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27LFM2-1.2B-Q4_0.gguf%3B+filename%3D%22LFM2-1.2B-Q4_0.gguf%22%3B&x-id=GetObject&Expires=1755884437&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTg4NDQzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODcyNGVhODdhNGU1ODVlNjk5ZGQ3ZmEvYjdmZmExZTZmZDc2NjY5ZDhjYjFmZGZhYWFkY2VhZTI1NWU2N2QzZjM0ODNjNmM0ZDkzN2EzOTRiYWZjNDQyZCoifV19&Signature=WtVUVHd90YiEGE%7EFa7X2Fz%7E6nbeJ%7ET6idpxvTZPEQuHuWEDse8Dx87i28T-2o%7ExFkTWJH4qwJHz9uBBhEqGYU1%7EFoNQOsirQ5GzTahaDC5aF7bR-Xy%7EVXSZM8TGFRVGvzjHCQeq2iDcCcsie7Nq0Q44vEput-DXija6N0RyM3aMOzK6n9yjGq4kRL202qzXcUbsRWo-06IwNu9FqfAMWFl4IffSGnC5KLF7jRw-iBUidPrw2109bfOL-KYe8o-Z31KM%7EDtieRTzqh1O-avyYCza2Pjqwl7gQrZQPK5KzGIPB2eZ195WI2I-L5Hw4Ji-i9-nVEbl97QKRIwyLN7x2bw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-22 16:40:37--  https://cas-bridge.xethub.hf.co/xet-bridge-us/68724ea87a4e585e699dd7fa/b7ffa1e6fd76669d8cb1fdfaaadceae255e67d3f3483c6c4d937a394bafc442d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250822%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250822T164037Z&X-Amz-Expires=3600&X-Amz-Signature=52326995326554ed45eb440039f1b0da57cc266192a1408a9534f69e47e5a008&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27LFM2-1.2B-Q4_0.gguf%3B+filename%3D%22LFM2-1.2B-Q4_0.gguf%22%3B&x-id=GetObject&Expires=1755884437&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTg4NDQzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODcyNGVhODdhNGU1ODVlNjk5ZGQ3ZmEvYjdmZmExZTZmZDc2NjY5ZDhjYjFmZGZhYWFkY2VhZTI1NWU2N2QzZjM0ODNjNmM0ZDkzN2EzOTRiYWZjNDQyZCoifV19&Signature=WtVUVHd90YiEGE%7EFa7X2Fz%7E6nbeJ%7ET6idpxvTZPEQuHuWEDse8Dx87i28T-2o%7ExFkTWJH4qwJHz9uBBhEqGYU1%7EFoNQOsirQ5GzTahaDC5aF7bR-Xy%7EVXSZM8TGFRVGvzjHCQeq2iDcCcsie7Nq0Q44vEput-DXija6N0RyM3aMOzK6n9yjGq4kRL202qzXcUbsRWo-06IwNu9FqfAMWFl4IffSGnC5KLF7jRw-iBUidPrw2109bfOL-KYe8o-Z31KM%7EDtieRTzqh1O-avyYCza2Pjqwl7gQrZQPK5KzGIPB2eZ195WI2I-L5Hw4Ji-i9-nVEbl97QKRIwyLN7x2bw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.167.56.38, 3.167.56.94, 3.167.56.110, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.167.56.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695749568 (664M)\n",
            "Saving to: â€˜LFM2-1.2B-Q4_0.ggufâ€™\n",
            "\n",
            "LFM2-1.2B-Q4_0.gguf 100%[===================>] 663.52M  77.5MB/s    in 8.6s    \n",
            "\n",
            "2025-08-22 16:40:45 (77.1 MB/s) - â€˜LFM2-1.2B-Q4_0.ggufâ€™ saved [695749568/695749568]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF/resolve/main/LFM2-1.2B-Q4_0.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "DCzitsnWI_eo",
        "outputId": "b2cd805b-cb4b-4cd2-aa1e-1338f3a0f708"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m \u001b[1;34mğŸ¤– LFM2 Chat Assistant\u001b[0m \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m \u001b[2mPowered by llama.cpp\u001b[0m   \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸ¤– LFM2 Chat Assistant</span> <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Powered by llama.cpp</span>   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mâ³ Loading model:\u001b[0m \u001b[2m.\u001b[0m\u001b[2;35m/\u001b[0m\u001b[2;95mLFM2-1.2B-Q4_0.gguf\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">â³ Loading model:</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">.</span><span style=\"color: #bf7fbf; text-decoration-color: #bf7fbf\">/</span><span style=\"color: #ff7fff; text-decoration-color: #ff7fff\">LFM2-1.2B-Q4_0.gguf</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mğŸ’¡ Use Ctrl+C to exit\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ğŸ’¡ Use Ctrl+C to exit</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mğŸ”§ System Prompt\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[3mYou're are helpful assistant. Be concise and accurate.\u001b[0m                                                          \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ”§ System Prompt</span><span style=\"color: #008080; text-decoration-color: #008080\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"font-style: italic\">You're are helpful assistant. Be concise and accurate.</span>                                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mğŸ‘¤ User:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸ‘¤ User:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3332911072.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mredirect_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[red]âŒ Error: {e}[/red]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3332911072.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# User input with nice formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[bold blue]ğŸ‘¤ User:[/bold blue] \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0muser_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from rich import print as rprint\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.text import Text\n",
        "from llama_cpp import Llama\n",
        "from contextlib import redirect_stderr\n",
        "\n",
        "console = Console()\n",
        "\n",
        "SYSTEM_PROMPT = \"You're are helpful assistant. Be concise and accurate.\"\n",
        "\n",
        "# Model file path. You can download the GGUF model files from https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF\n",
        "### CHANGE THIS TO THE MODEL YOU WANT TO USE ###\n",
        "MODEL_GGUF_FILE = \"./LFM2-1.2B-Q4_0.gguf\"\n",
        "################################################\n",
        "\n",
        "def main():\n",
        "    # Welcome panel\n",
        "    console.print(Panel.fit(\n",
        "        \"[bold blue]ğŸ¤– LFM2 Chat Assistant[/bold blue]\\n[dim]Powered by llama.cpp[/dim]\",\n",
        "        border_style=\"blue\"\n",
        "    ))\n",
        "\n",
        "    # Loading model\n",
        "    console.print(f\"[yellow]â³ Loading model:[/yellow] [dim]{MODEL_GGUF_FILE}[/dim]\")\n",
        "    llm = Llama(\n",
        "        model_path=MODEL_GGUF_FILE,\n",
        "    )\n",
        "    console.print(f\"[dim]ğŸ’¡ Use Ctrl+C to exit[/dim]\")\n",
        "    console.print()\n",
        "\n",
        "    # Maintain the chat history in this variable\n",
        "    history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "\n",
        "    # System prompt panel\n",
        "    system_panel = Panel(\n",
        "        f\"[italic]{SYSTEM_PROMPT}[/italic]\",\n",
        "        title=\"[bold cyan]ğŸ”§ System Prompt[/bold cyan]\",\n",
        "        border_style=\"cyan\",\n",
        "        padding=(0, 1)\n",
        "    )\n",
        "    console.print(system_panel)\n",
        "    console.print()\n",
        "\n",
        "    while True:\n",
        "        # User input with nice formatting\n",
        "        console.print(\"[bold blue]ğŸ‘¤ User:[/bold blue] \", end=\"\")\n",
        "        user_prompt = input()\n",
        "\n",
        "        if not user_prompt.strip():\n",
        "            continue\n",
        "\n",
        "        history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "        # Assistant response header\n",
        "        console.print(\"\\n[bold green]ğŸ¤– Assistant:[/bold green]\")\n",
        "\n",
        "        # Generate content with the chat history\n",
        "        output = llm.create_chat_completion(messages=history)\n",
        "        generated_message = output[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "        # Display response in a panel\n",
        "        response_panel = Panel(\n",
        "            f\"[white]{generated_message}[/white]\",\n",
        "            border_style=\"green\",\n",
        "            padding=(0, 1)\n",
        "        )\n",
        "        console.print(response_panel)\n",
        "        console.print()\n",
        "\n",
        "        # Finally, push the assistant message back as part of the history.\n",
        "        history.append({\"role\": \"assistant\", \"content\": generated_message})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Redirect stderr output to a file to avoid confusing the user\n",
        "    with open(\"llamacpp.log\", \"w\") as f:\n",
        "        with redirect_stderr(f):\n",
        "            try:\n",
        "                main()\n",
        "            except Exception as e:\n",
        "                console.print(f\"\\n[red]âŒ Error: {e}[/red]\")\n",
        "                sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WRd-SJ0cqoEC",
        "outputId": "77ad7d9e-6c1d-4671-c099-76f1a2dc6ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 34 key-value pairs and 148 tensors from ./LFM2-1.2B-Q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = lfm2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = LFM2 1.2B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = LFM2\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 1.2B\n",
            "llama_model_loader: - kv   5:                            general.license str              = other\n",
            "llama_model_loader: - kv   6:                       general.license.name str              = lfm1.0\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = LICENSE\n",
            "llama_model_loader: - kv   8:                               general.tags arr[str,4]       = [\"liquid\", \"lfm2\", \"edge\", \"text-gene...\n",
            "llama_model_loader: - kv   9:                          general.languages arr[str,8]       = [\"en\", \"ar\", \"zh\", \"fr\", \"de\", \"ja\", ...\n",
            "llama_model_loader: - kv  10:                           lfm2.block_count u32              = 16\n",
            "llama_model_loader: - kv  11:                        lfm2.context_length u32              = 128000\n",
            "llama_model_loader: - kv  12:                      lfm2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  13:                   lfm2.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                  lfm2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  15:               lfm2.attention.head_count_kv arr[i32,16]      = [0, 0, 8, 0, 0, 8, 0, 0, 8, 0, 8, 0, ...\n",
            "llama_model_loader: - kv  16:                        lfm2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  17:                            lfm2.vocab_size u32              = 65536\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-25' coro=<Server.serve() done, defined at /usr/local/lib/python3.12/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "llama_model_loader: - kv  18:                     lfm2.shortconv.l_cache u32              = 3\n",
            "llama_model_loader: - kv  19:      lfm2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = lfm2\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,65536]   = [\"<|pad|>\", \"<|startoftext|>\", \"<|end...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,65536]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "Exception ignored on calling ctypes callback function: <function llama_log_callback at 0x7eaa56892160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/llama_cpp/_logger.py\", line 39, in llama_log_callback\n",
            "    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 128: invalid continuation byte\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 7\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
            "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  33:                          general.file_type u32              = 2\n",
            "llama_model_loader: - type  f32:   55 tensors\n",
            "llama_model_loader: - type q4_0:   92 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 661.25 MiB (4.74 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token:  64398 '<|file_start|>' is not marked as EOG\n",
            "load: control token:  64396 '<|review_start|>' is not marked as EOG\n",
            "load: control token:  64395 '<|cot_end|>' is not marked as EOG\n",
            "load: control token:  64397 '<|review_end|>' is not marked as EOG\n",
            "load: control token:    359 '<|reserved_349|>' is not marked as EOG\n",
            "load: control token:    158 '<|reserved_148|>' is not marked as EOG\n",
            "load: control token:    363 '<|reserved_353|>' is not marked as EOG\n",
            "load: control token:    178 '<|reserved_168|>' is not marked as EOG\n",
            "load: control token:    121 '<|reserved_111|>' is not marked as EOG\n",
            "load: control token:     19 '<|reserved_9|>' is not marked as EOG\n",
            "load: control token:    430 '<|reserved_420|>' is not marked as EOG\n",
            "load: control token:    423 '<|reserved_413|>' is not marked as EOG\n",
            "load: control token:    107 '<|reserved_97|>' is not marked as EOG\n",
            "load: control token:    393 '<|reserved_383|>' is not marked as EOG\n",
            "load: control token:    397 '<|reserved_387|>' is not marked as EOG\n",
            "load: control token:    206 '<|reserved_196|>' is not marked as EOG\n",
            "load: control token:    314 '<|reserved_304|>' is not marked as EOG\n",
            "load: control token:     31 '<|reserved_21|>' is not marked as EOG\n",
            "load: control token:    195 '<|reserved_185|>' is not marked as EOG\n",
            "load: control token:    134 '<|reserved_124|>' is not marked as EOG\n",
            "load: control token:    140 '<|reserved_130|>' is not marked as EOG\n",
            "load: control token:     48 '<|reserved_38|>' is not marked as EOG\n",
            "load: control token:    444 '<|reserved_434|>' is not marked as EOG\n",
            "load: control token:     69 '<|reserved_59|>' is not marked as EOG\n",
            "load: control token:    354 '<|reserved_344|>' is not marked as EOG\n",
            "load: control token:     12 '<|tool_response_start|>' is not marked as EOG\n",
            "load: control token:     37 '<|reserved_27|>' is not marked as EOG\n",
            "load: control token:    398 '<|reserved_388|>' is not marked as EOG\n",
            "load: control token:    241 '<|reserved_231|>' is not marked as EOG\n",
            "load: control token:    390 '<|reserved_380|>' is not marked as EOG\n",
            "load: control token:    220 '<|reserved_210|>' is not marked as EOG\n",
            "load: control token:    177 '<|reserved_167|>' is not marked as EOG\n",
            "load: control token:    415 '<|reserved_405|>' is not marked as EOG\n",
            "load: control token:    381 '<|reserved_371|>' is not marked as EOG\n",
            "load: control token:     99 '<|reserved_89|>' is not marked as EOG\n",
            "load: control token:     11 '<|tool_call_end|>' is not marked as EOG\n",
            "load: control token:    136 '<|reserved_126|>' is not marked as EOG\n",
            "load: control token:     78 '<|reserved_68|>' is not marked as EOG\n",
            "load: control token:     23 '<|reserved_13|>' is not marked as EOG\n",
            "load: control token:    250 '<|reserved_240|>' is not marked as EOG\n",
            "load: control token:    500 '<|reserved_490|>' is not marked as EOG\n",
            "load: control token:    259 '<|reserved_249|>' is not marked as EOG\n",
            "load: control token:    339 '<|reserved_329|>' is not marked as EOG\n",
            "load: control token:     65 '<|reserved_55|>' is not marked as EOG\n",
            "load: control token:    313 '<|reserved_303|>' is not marked as EOG\n",
            "load: control token:    192 '<|reserved_182|>' is not marked as EOG\n",
            "load: control token:    499 '<|reserved_489|>' is not marked as EOG\n",
            "load: control token:  64399 '<|file_end|>' is not marked as EOG\n",
            "load: control token:    170 '<|reserved_160|>' is not marked as EOG\n",
            "load: control token:    388 '<|reserved_378|>' is not marked as EOG\n",
            "load: control token:    358 '<|reserved_348|>' is not marked as EOG\n",
            "load: control token:     49 '<|reserved_39|>' is not marked as EOG\n",
            "load: control token:    173 '<|reserved_163|>' is not marked as EOG\n",
            "load: control token:    472 '<|reserved_462|>' is not marked as EOG\n",
            "load: control token:    219 '<|reserved_209|>' is not marked as EOG\n",
            "load: control token:    137 '<|reserved_127|>' is not marked as EOG\n",
            "load: control token:    124 '<|reserved_114|>' is not marked as EOG\n",
            "load: control token:    196 '<|reserved_186|>' is not marked as EOG\n",
            "load: control token:    253 '<|reserved_243|>' is not marked as EOG\n",
            "load: control token:    112 '<|reserved_102|>' is not marked as EOG\n",
            "load: control token:    100 '<|reserved_90|>' is not marked as EOG\n",
            "load: control token:    342 '<|reserved_332|>' is not marked as EOG\n",
            "load: control token:     26 '<|reserved_16|>' is not marked as EOG\n",
            "load: control token:    460 '<|reserved_450|>' is not marked as EOG\n",
            "load: control token:    437 '<|reserved_427|>' is not marked as EOG\n",
            "load: control token:    186 '<|reserved_176|>' is not marked as EOG\n",
            "load: control token:    244 '<|reserved_234|>' is not marked as EOG\n",
            "load: control token:    281 '<|reserved_271|>' is not marked as EOG\n",
            "load: control token:    273 '<|reserved_263|>' is not marked as EOG\n",
            "load: control token:    257 '<|reserved_247|>' is not marked as EOG\n",
            "load: control token:    256 '<|reserved_246|>' is not marked as EOG\n",
            "load: control token:    182 '<|reserved_172|>' is not marked as EOG\n",
            "load: control token:    492 '<|reserved_482|>' is not marked as EOG\n",
            "load: control token:    153 '<|reserved_143|>' is not marked as EOG\n",
            "load: control token:    455 '<|reserved_445|>' is not marked as EOG\n",
            "load: control token:    249 '<|reserved_239|>' is not marked as EOG\n",
            "load: control token:    226 '<|reserved_216|>' is not marked as EOG\n",
            "load: control token:    461 '<|reserved_451|>' is not marked as EOG\n",
            "load: control token:    302 '<|reserved_292|>' is not marked as EOG\n",
            "load: control token:    442 '<|reserved_432|>' is not marked as EOG\n",
            "load: control token:     10 '<|tool_call_start|>' is not marked as EOG\n",
            "load: control token:    462 '<|reserved_452|>' is not marked as EOG\n",
            "load: control token:    169 '<|reserved_159|>' is not marked as EOG\n",
            "load: control token:    202 '<|reserved_192|>' is not marked as EOG\n",
            "load: control token:     36 '<|reserved_26|>' is not marked as EOG\n",
            "load: control token:    491 '<|reserved_481|>' is not marked as EOG\n",
            "load: control token:    265 '<|reserved_255|>' is not marked as EOG\n",
            "load: control token:    203 '<|reserved_193|>' is not marked as EOG\n",
            "load: control token:    295 '<|reserved_285|>' is not marked as EOG\n",
            "load: control token:    223 '<|reserved_213|>' is not marked as EOG\n",
            "load: control token:    422 '<|reserved_412|>' is not marked as EOG\n",
            "load: control token:     52 '<|reserved_42|>' is not marked as EOG\n",
            "load: control token:    116 '<|reserved_106|>' is not marked as EOG\n",
            "load: control token:     27 '<|reserved_17|>' is not marked as EOG\n",
            "load: control token:    228 '<|reserved_218|>' is not marked as EOG\n",
            "load: control token:    429 '<|reserved_419|>' is not marked as EOG\n",
            "load: control token:     66 '<|reserved_56|>' is not marked as EOG\n",
            "load: control token:    316 '<|reserved_306|>' is not marked as EOG\n",
            "load: control token:    271 '<|reserved_261|>' is not marked as EOG\n",
            "load: control token:    209 '<|reserved_199|>' is not marked as EOG\n",
            "load: control token:    382 '<|reserved_372|>' is not marked as EOG\n",
            "load: control token:    261 '<|reserved_251|>' is not marked as EOG\n",
            "load: control token:    222 '<|reserved_212|>' is not marked as EOG\n",
            "load: control token:    412 '<|reserved_402|>' is not marked as EOG\n",
            "load: control token:    385 '<|reserved_375|>' is not marked as EOG\n",
            "load: control token:    225 '<|reserved_215|>' is not marked as EOG\n",
            "load: control token:    204 '<|reserved_194|>' is not marked as EOG\n",
            "load: control token:    200 '<|reserved_190|>' is not marked as EOG\n",
            "load: control token:     45 '<|reserved_35|>' is not marked as EOG\n",
            "load: control token:    449 '<|reserved_439|>' is not marked as EOG\n",
            "load: control token:    227 '<|reserved_217|>' is not marked as EOG\n",
            "load: control token:    161 '<|reserved_151|>' is not marked as EOG\n",
            "load: control token:    341 '<|reserved_331|>' is not marked as EOG\n",
            "load: control token:    443 '<|reserved_433|>' is not marked as EOG\n",
            "load: control token:    488 '<|reserved_478|>' is not marked as EOG\n",
            "load: control token:    298 '<|reserved_288|>' is not marked as EOG\n",
            "load: control token:    408 '<|reserved_398|>' is not marked as EOG\n",
            "load: control token:    145 '<|reserved_135|>' is not marked as EOG\n",
            "load: control token:    201 '<|reserved_191|>' is not marked as EOG\n",
            "load: control token:    343 '<|reserved_333|>' is not marked as EOG\n",
            "load: control token:    255 '<|reserved_245|>' is not marked as EOG\n",
            "load: control token:    297 '<|reserved_287|>' is not marked as EOG\n",
            "load: control token:      9 '<|tool_list_end|>' is not marked as EOG\n",
            "load: control token:    123 '<|reserved_113|>' is not marked as EOG\n",
            "load: control token:    340 '<|reserved_330|>' is not marked as EOG\n",
            "load: control token:    487 '<|reserved_477|>' is not marked as EOG\n",
            "load: control token:     42 '<|reserved_32|>' is not marked as EOG\n",
            "load: control token:      3 '<|fim_pre|>' is not marked as EOG\n",
            "load: control token:    468 '<|reserved_458|>' is not marked as EOG\n",
            "load: control token:     29 '<|reserved_19|>' is not marked as EOG\n",
            "load: control token:    175 '<|reserved_165|>' is not marked as EOG\n",
            "load: control token:    451 '<|reserved_441|>' is not marked as EOG\n",
            "load: control token:    446 '<|reserved_436|>' is not marked as EOG\n",
            "load: control token:    416 '<|reserved_406|>' is not marked as EOG\n",
            "load: control token:    240 '<|reserved_230|>' is not marked as EOG\n",
            "load: control token:    387 '<|reserved_377|>' is not marked as EOG\n",
            "load: control token:    111 '<|reserved_101|>' is not marked as EOG\n",
            "load: control token:     73 '<|reserved_63|>' is not marked as EOG\n",
            "load: control token:    426 '<|reserved_416|>' is not marked as EOG\n",
            "load: control token:      8 '<|tool_list_start|>' is not marked as EOG\n",
            "load: control token:     47 '<|reserved_37|>' is not marked as EOG\n",
            "load: control token:    361 '<|reserved_351|>' is not marked as EOG\n",
            "load: control token:    289 '<|reserved_279|>' is not marked as EOG\n",
            "load: control token:      5 '<|fim_suf|>' is not marked as EOG\n",
            "load: control token:    392 '<|reserved_382|>' is not marked as EOG\n",
            "load: control token:     56 '<|reserved_46|>' is not marked as EOG\n",
            "load: control token:    470 '<|reserved_460|>' is not marked as EOG\n",
            "load: control token:    498 '<|reserved_488|>' is not marked as EOG\n",
            "load: control token:    438 '<|reserved_428|>' is not marked as EOG\n",
            "load: control token:     59 '<|reserved_49|>' is not marked as EOG\n",
            "load: control token:    127 '<|reserved_117|>' is not marked as EOG\n",
            "load: control token:    399 '<|reserved_389|>' is not marked as EOG\n",
            "load: control token:     91 '<|reserved_81|>' is not marked as EOG\n",
            "load: control token:    101 '<|reserved_91|>' is not marked as EOG\n",
            "load: control token:     35 '<|reserved_25|>' is not marked as EOG\n",
            "load: control token:    466 '<|reserved_456|>' is not marked as EOG\n",
            "load: control token:     81 '<|reserved_71|>' is not marked as EOG\n",
            "load: control token:    432 '<|reserved_422|>' is not marked as EOG\n",
            "load: control token:  64394 '<|cot_start|>' is not marked as EOG\n",
            "load: control token:     82 '<|reserved_72|>' is not marked as EOG\n",
            "load: control token:    480 '<|reserved_470|>' is not marked as EOG\n",
            "load: control token:    362 '<|reserved_352|>' is not marked as EOG\n",
            "load: control token:    242 '<|reserved_232|>' is not marked as EOG\n",
            "load: control token:    181 '<|reserved_171|>' is not marked as EOG\n",
            "load: control token:    239 '<|reserved_229|>' is not marked as EOG\n",
            "load: control token:    384 '<|reserved_374|>' is not marked as EOG\n",
            "load: control token:    471 '<|reserved_461|>' is not marked as EOG\n",
            "load: control token:    235 '<|reserved_225|>' is not marked as EOG\n",
            "load: control token:    103 '<|reserved_93|>' is not marked as EOG\n",
            "load: control token:    406 '<|reserved_396|>' is not marked as EOG\n",
            "load: control token:    275 '<|reserved_265|>' is not marked as EOG\n",
            "load: control token:    278 '<|reserved_268|>' is not marked as EOG\n",
            "load: control token:    119 '<|reserved_109|>' is not marked as EOG\n",
            "load: control token:    328 '<|reserved_318|>' is not marked as EOG\n",
            "load: control token:    279 '<|reserved_269|>' is not marked as EOG\n",
            "load: control token:    484 '<|reserved_474|>' is not marked as EOG\n",
            "load: control token:    149 '<|reserved_139|>' is not marked as EOG\n",
            "load: control token:    185 '<|reserved_175|>' is not marked as EOG\n",
            "load: control token:    373 '<|reserved_363|>' is not marked as EOG\n",
            "load: control token:     46 '<|reserved_36|>' is not marked as EOG\n",
            "load: control token:     21 '<|reserved_11|>' is not marked as EOG\n",
            "load: control token:    409 '<|reserved_399|>' is not marked as EOG\n",
            "load: control token:    122 '<|reserved_112|>' is not marked as EOG\n",
            "load: control token:    394 '<|reserved_384|>' is not marked as EOG\n",
            "load: control token:    165 '<|reserved_155|>' is not marked as EOG\n",
            "load: control token:    400 '<|reserved_390|>' is not marked as EOG\n",
            "load: control token:    270 '<|reserved_260|>' is not marked as EOG\n",
            "load: control token:    132 '<|reserved_122|>' is not marked as EOG\n",
            "load: control token:    440 '<|reserved_430|>' is not marked as EOG\n",
            "load: control token:     96 '<|reserved_86|>' is not marked as EOG\n",
            "load: control token:    233 '<|reserved_223|>' is not marked as EOG\n",
            "load: control token:    141 '<|reserved_131|>' is not marked as EOG\n",
            "load: control token:    130 '<|reserved_120|>' is not marked as EOG\n",
            "load: control token:    205 '<|reserved_195|>' is not marked as EOG\n",
            "load: control token:     57 '<|reserved_47|>' is not marked as EOG\n",
            "load: control token:    463 '<|reserved_453|>' is not marked as EOG\n",
            "load: control token:    475 '<|reserved_465|>' is not marked as EOG\n",
            "load: control token:    483 '<|reserved_473|>' is not marked as EOG\n",
            "load: control token:    272 '<|reserved_262|>' is not marked as EOG\n",
            "load: control token:    229 '<|reserved_219|>' is not marked as EOG\n",
            "load: control token:     83 '<|reserved_73|>' is not marked as EOG\n",
            "load: control token:    143 '<|reserved_133|>' is not marked as EOG\n",
            "load: control token:     43 '<|reserved_33|>' is not marked as EOG\n",
            "load: control token:    108 '<|reserved_98|>' is not marked as EOG\n",
            "load: control token:    357 '<|reserved_347|>' is not marked as EOG\n",
            "load: control token:    396 '<|reserved_386|>' is not marked as EOG\n",
            "load: control token:    199 '<|reserved_189|>' is not marked as EOG\n",
            "load: control token:    496 '<|reserved_486|>' is not marked as EOG\n",
            "load: control token:      6 '<|im_start|>' is not marked as EOG\n",
            "load: control token:    221 '<|reserved_211|>' is not marked as EOG\n",
            "load: control token:    370 '<|reserved_360|>' is not marked as EOG\n",
            "load: control token:    277 '<|reserved_267|>' is not marked as EOG\n",
            "load: control token:    135 '<|reserved_125|>' is not marked as EOG\n",
            "load: control token:    439 '<|reserved_429|>' is not marked as EOG\n",
            "load: control token:    243 '<|reserved_233|>' is not marked as EOG\n",
            "load: control token:    497 '<|reserved_487|>' is not marked as EOG\n",
            "load: control token:    167 '<|reserved_157|>' is not marked as EOG\n",
            "load: control token:    156 '<|reserved_146|>' is not marked as EOG\n",
            "load: control token:    424 '<|reserved_414|>' is not marked as EOG\n",
            "load: control token:    258 '<|reserved_248|>' is not marked as EOG\n",
            "load: control token:    263 '<|reserved_253|>' is not marked as EOG\n",
            "load: control token:    309 '<|reserved_299|>' is not marked as EOG\n",
            "load: control token:    300 '<|reserved_290|>' is not marked as EOG\n",
            "load: control token:    332 '<|reserved_322|>' is not marked as EOG\n",
            "load: control token:     79 '<|reserved_69|>' is not marked as EOG\n",
            "load: control token:    326 '<|reserved_316|>' is not marked as EOG\n",
            "load: control token:    431 '<|reserved_421|>' is not marked as EOG\n",
            "load: control token:    283 '<|reserved_273|>' is not marked as EOG\n",
            "load: control token:    264 '<|reserved_254|>' is not marked as EOG\n",
            "load: control token:     71 '<|reserved_61|>' is not marked as EOG\n",
            "load: control token:     25 '<|reserved_15|>' is not marked as EOG\n",
            "load: control token:    113 '<|reserved_103|>' is not marked as EOG\n",
            "load: control token:     74 '<|reserved_64|>' is not marked as EOG\n",
            "load: control token:     38 '<|reserved_28|>' is not marked as EOG\n",
            "load: control token:    349 '<|reserved_339|>' is not marked as EOG\n",
            "load: control token:    493 '<|reserved_483|>' is not marked as EOG\n",
            "load: control token:    131 '<|reserved_121|>' is not marked as EOG\n",
            "load: control token:    157 '<|reserved_147|>' is not marked as EOG\n",
            "load: control token:    236 '<|reserved_226|>' is not marked as EOG\n",
            "load: control token:    293 '<|reserved_283|>' is not marked as EOG\n",
            "load: control token:     80 '<|reserved_70|>' is not marked as EOG\n",
            "load: control token:     85 '<|reserved_75|>' is not marked as EOG\n",
            "load: control token:    179 '<|reserved_169|>' is not marked as EOG\n",
            "load: control token:    418 '<|reserved_408|>' is not marked as EOG\n",
            "load: control token:    360 '<|reserved_350|>' is not marked as EOG\n",
            "load: control token:    401 '<|reserved_391|>' is not marked as EOG\n",
            "load: control token:    133 '<|reserved_123|>' is not marked as EOG\n",
            "load: control token:     60 '<|reserved_50|>' is not marked as EOG\n",
            "load: control token:    114 '<|reserved_104|>' is not marked as EOG\n",
            "load: control token:    190 '<|reserved_180|>' is not marked as EOG\n",
            "load: control token:     70 '<|reserved_60|>' is not marked as EOG\n",
            "load: control token:    218 '<|reserved_208|>' is not marked as EOG\n",
            "load: control token:    260 '<|reserved_250|>' is not marked as EOG\n",
            "load: control token:    269 '<|reserved_259|>' is not marked as EOG\n",
            "load: control token:    282 '<|reserved_272|>' is not marked as EOG\n",
            "load: control token:    284 '<|reserved_274|>' is not marked as EOG\n",
            "load: control token:    315 '<|reserved_305|>' is not marked as EOG\n",
            "load: control token:    346 '<|reserved_336|>' is not marked as EOG\n",
            "load: control token:    317 '<|reserved_307|>' is not marked as EOG\n",
            "load: control token:    448 '<|reserved_438|>' is not marked as EOG\n",
            "load: control token:    296 '<|reserved_286|>' is not marked as EOG\n",
            "load: control token:     64 '<|reserved_54|>' is not marked as EOG\n",
            "load: control token:     87 '<|reserved_77|>' is not marked as EOG\n",
            "load: control token:    248 '<|reserved_238|>' is not marked as EOG\n",
            "load: control token:    286 '<|reserved_276|>' is not marked as EOG\n",
            "load: control token:    351 '<|reserved_341|>' is not marked as EOG\n",
            "load: control token:    299 '<|reserved_289|>' is not marked as EOG\n",
            "load: control token:    303 '<|reserved_293|>' is not marked as EOG\n",
            "load: control token:    118 '<|reserved_108|>' is not marked as EOG\n",
            "load: control token:    380 '<|reserved_370|>' is not marked as EOG\n",
            "load: control token:    146 '<|reserved_136|>' is not marked as EOG\n",
            "load: control token:    188 '<|reserved_178|>' is not marked as EOG\n",
            "load: control token:    311 '<|reserved_301|>' is not marked as EOG\n",
            "load: control token:    312 '<|reserved_302|>' is not marked as EOG\n",
            "load: control token:    427 '<|reserved_417|>' is not marked as EOG\n",
            "load: control token:    435 '<|reserved_425|>' is not marked as EOG\n",
            "load: control token:    322 '<|reserved_312|>' is not marked as EOG\n",
            "load: control token:    290 '<|reserved_280|>' is not marked as EOG\n",
            "load: control token:     90 '<|reserved_80|>' is not marked as EOG\n",
            "load: control token:    428 '<|reserved_418|>' is not marked as EOG\n",
            "load: control token:     54 '<|reserved_44|>' is not marked as EOG\n",
            "load: control token:    457 '<|reserved_447|>' is not marked as EOG\n",
            "load: control token:     92 '<|reserved_82|>' is not marked as EOG\n",
            "load: control token:    262 '<|reserved_252|>' is not marked as EOG\n",
            "load: control token:    450 '<|reserved_440|>' is not marked as EOG\n",
            "load: control token:    291 '<|reserved_281|>' is not marked as EOG\n",
            "load: control token:    217 '<|reserved_207|>' is not marked as EOG\n",
            "load: control token:    276 '<|reserved_266|>' is not marked as EOG\n",
            "load: control token:    441 '<|reserved_431|>' is not marked as EOG\n",
            "load: control token:    353 '<|reserved_343|>' is not marked as EOG\n",
            "load: control token:    230 '<|reserved_220|>' is not marked as EOG\n",
            "load: control token:    376 '<|reserved_366|>' is not marked as EOG\n",
            "load: control token:    411 '<|reserved_401|>' is not marked as EOG\n",
            "load: control token:    155 '<|reserved_145|>' is not marked as EOG\n",
            "load: control token:    105 '<|reserved_95|>' is not marked as EOG\n",
            "load: control token:    126 '<|reserved_116|>' is not marked as EOG\n",
            "load: control token:    368 '<|reserved_358|>' is not marked as EOG\n",
            "load: control token:    166 '<|reserved_156|>' is not marked as EOG\n",
            "load: control token:     51 '<|reserved_41|>' is not marked as EOG\n",
            "load: control token:    371 '<|reserved_361|>' is not marked as EOG\n",
            "load: control token:    372 '<|reserved_362|>' is not marked as EOG\n",
            "load: control token:    453 '<|reserved_443|>' is not marked as EOG\n",
            "load: control token:    285 '<|reserved_275|>' is not marked as EOG\n",
            "load: control token:     72 '<|reserved_62|>' is not marked as EOG\n",
            "load: control token:    383 '<|reserved_373|>' is not marked as EOG\n",
            "load: control token:    347 '<|reserved_337|>' is not marked as EOG\n",
            "load: control token:    436 '<|reserved_426|>' is not marked as EOG\n",
            "load: control token:    115 '<|reserved_105|>' is not marked as EOG\n",
            "load: control token:    476 '<|reserved_466|>' is not marked as EOG\n",
            "load: control token:    355 '<|reserved_345|>' is not marked as EOG\n",
            "load: control token:    459 '<|reserved_449|>' is not marked as EOG\n",
            "load: control token:    231 '<|reserved_221|>' is not marked as EOG\n",
            "load: control token:    142 '<|reserved_132|>' is not marked as EOG\n",
            "load: control token:    485 '<|reserved_475|>' is not marked as EOG\n",
            "load: control token:    445 '<|reserved_435|>' is not marked as EOG\n",
            "load: control token:    489 '<|reserved_479|>' is not marked as EOG\n",
            "load: control token:    344 '<|reserved_334|>' is not marked as EOG\n",
            "load: control token:     24 '<|reserved_14|>' is not marked as EOG\n",
            "load: control token:    237 '<|reserved_227|>' is not marked as EOG\n",
            "load: control token:     84 '<|reserved_74|>' is not marked as EOG\n",
            "load: control token:    164 '<|reserved_154|>' is not marked as EOG\n",
            "load: control token:    327 '<|reserved_317|>' is not marked as EOG\n",
            "load: control token:    335 '<|reserved_325|>' is not marked as EOG\n",
            "load: control token:    102 '<|reserved_92|>' is not marked as EOG\n",
            "load: control token:     98 '<|reserved_88|>' is not marked as EOG\n",
            "load: control token:     41 '<|reserved_31|>' is not marked as EOG\n",
            "load: control token:    481 '<|reserved_471|>' is not marked as EOG\n",
            "load: control token:    254 '<|reserved_244|>' is not marked as EOG\n",
            "load: control token:     95 '<|reserved_85|>' is not marked as EOG\n",
            "load: control token:    467 '<|reserved_457|>' is not marked as EOG\n",
            "load: control token:    128 '<|reserved_118|>' is not marked as EOG\n",
            "load: control token:    224 '<|reserved_214|>' is not marked as EOG\n",
            "load: control token:    330 '<|reserved_320|>' is not marked as EOG\n",
            "load: control token:     68 '<|reserved_58|>' is not marked as EOG\n",
            "load: control token:    151 '<|reserved_141|>' is not marked as EOG\n",
            "load: control token:    350 '<|reserved_340|>' is not marked as EOG\n",
            "load: control token:    402 '<|reserved_392|>' is not marked as EOG\n",
            "load: control token:    306 '<|reserved_296|>' is not marked as EOG\n",
            "load: control token:    365 '<|reserved_355|>' is not marked as EOG\n",
            "load: control token:    477 '<|reserved_467|>' is not marked as EOG\n",
            "load: control token:    266 '<|reserved_256|>' is not marked as EOG\n",
            "load: control token:    486 '<|reserved_476|>' is not marked as EOG\n",
            "load: control token:     33 '<|reserved_23|>' is not marked as EOG\n",
            "load: control token:    345 '<|reserved_335|>' is not marked as EOG\n",
            "load: control token:     34 '<|reserved_24|>' is not marked as EOG\n",
            "load: control token:     30 '<|reserved_20|>' is not marked as EOG\n",
            "load: control token:    367 '<|reserved_357|>' is not marked as EOG\n",
            "load: control token:    403 '<|reserved_393|>' is not marked as EOG\n",
            "load: control token:    245 '<|reserved_235|>' is not marked as EOG\n",
            "load: control token:    193 '<|reserved_183|>' is not marked as EOG\n",
            "load: control token:    321 '<|reserved_311|>' is not marked as EOG\n",
            "load: control token:    301 '<|reserved_291|>' is not marked as EOG\n",
            "load: control token:    320 '<|reserved_310|>' is not marked as EOG\n",
            "load: control token:    246 '<|reserved_236|>' is not marked as EOG\n",
            "load: control token:    405 '<|reserved_395|>' is not marked as EOG\n",
            "load: control token:    337 '<|reserved_327|>' is not marked as EOG\n",
            "load: control token:     13 '<|tool_response_end|>' is not marked as EOG\n",
            "load: control token:    414 '<|reserved_404|>' is not marked as EOG\n",
            "load: control token:    215 '<|reserved_205|>' is not marked as EOG\n",
            "load: control token:    356 '<|reserved_346|>' is not marked as EOG\n",
            "load: control token:    159 '<|reserved_149|>' is not marked as EOG\n",
            "load: control token:    377 '<|reserved_367|>' is not marked as EOG\n",
            "load: control token:    494 '<|reserved_484|>' is not marked as EOG\n",
            "load: control token:    216 '<|reserved_206|>' is not marked as EOG\n",
            "load: control token:    144 '<|reserved_134|>' is not marked as EOG\n",
            "load: control token:    288 '<|reserved_278|>' is not marked as EOG\n",
            "load: control token:    162 '<|reserved_152|>' is not marked as EOG\n",
            "load: control token:    238 '<|reserved_228|>' is not marked as EOG\n",
            "load: control token:    374 '<|reserved_364|>' is not marked as EOG\n",
            "load: control token:     63 '<|reserved_53|>' is not marked as EOG\n",
            "load: control token:    420 '<|reserved_410|>' is not marked as EOG\n",
            "load: control token:    482 '<|reserved_472|>' is not marked as EOG\n",
            "load: control token:    214 '<|reserved_204|>' is not marked as EOG\n",
            "load: control token:    417 '<|reserved_407|>' is not marked as EOG\n",
            "load: control token:     39 '<|reserved_29|>' is not marked as EOG\n",
            "load: control token:    425 '<|reserved_415|>' is not marked as EOG\n",
            "load: control token:     77 '<|reserved_67|>' is not marked as EOG\n",
            "load: control token:     94 '<|reserved_84|>' is not marked as EOG\n",
            "load: control token:     50 '<|reserved_40|>' is not marked as EOG\n",
            "load: control token:    319 '<|reserved_309|>' is not marked as EOG\n",
            "load: control token:    171 '<|reserved_161|>' is not marked as EOG\n",
            "load: control token:    318 '<|reserved_308|>' is not marked as EOG\n",
            "load: control token:     44 '<|reserved_34|>' is not marked as EOG\n",
            "load: control token:    207 '<|reserved_197|>' is not marked as EOG\n",
            "load: control token:    274 '<|reserved_264|>' is not marked as EOG\n",
            "load: control token:    120 '<|reserved_110|>' is not marked as EOG\n",
            "load: control token:    106 '<|reserved_96|>' is not marked as EOG\n",
            "load: control token:     28 '<|reserved_18|>' is not marked as EOG\n",
            "load: control token:     18 '<|reserved_8|>' is not marked as EOG\n",
            "load: control token:    465 '<|reserved_455|>' is not marked as EOG\n",
            "load: control token:    109 '<|reserved_99|>' is not marked as EOG\n",
            "load: control token:    474 '<|reserved_464|>' is not marked as EOG\n",
            "load: control token:    379 '<|reserved_369|>' is not marked as EOG\n",
            "load: control token:     17 '<|reserved_7|>' is not marked as EOG\n",
            "load: control token:    464 '<|reserved_454|>' is not marked as EOG\n",
            "load: control token:    307 '<|reserved_297|>' is not marked as EOG\n",
            "load: control token:    310 '<|reserved_300|>' is not marked as EOG\n",
            "load: control token:    187 '<|reserved_177|>' is not marked as EOG\n",
            "load: control token:    125 '<|reserved_115|>' is not marked as EOG\n",
            "load: control token:    336 '<|reserved_326|>' is not marked as EOG\n",
            "load: control token:    410 '<|reserved_400|>' is not marked as EOG\n",
            "load: control token:    154 '<|reserved_144|>' is not marked as EOG\n",
            "load: control token:    180 '<|reserved_170|>' is not marked as EOG\n",
            "load: control token:     53 '<|reserved_43|>' is not marked as EOG\n",
            "load: control token:     22 '<|reserved_12|>' is not marked as EOG\n",
            "load: control token:    147 '<|reserved_137|>' is not marked as EOG\n",
            "load: control token:    172 '<|reserved_162|>' is not marked as EOG\n",
            "load: control token:    454 '<|reserved_444|>' is not marked as EOG\n",
            "load: control token:    292 '<|reserved_282|>' is not marked as EOG\n",
            "load: control token:    234 '<|reserved_224|>' is not marked as EOG\n",
            "load: control token:     55 '<|reserved_45|>' is not marked as EOG\n",
            "load: control token:    407 '<|reserved_397|>' is not marked as EOG\n",
            "load: control token:    152 '<|reserved_142|>' is not marked as EOG\n",
            "load: control token:    189 '<|reserved_179|>' is not marked as EOG\n",
            "load: control token:    198 '<|reserved_188|>' is not marked as EOG\n",
            "load: control token:     61 '<|reserved_51|>' is not marked as EOG\n",
            "load: control token:    456 '<|reserved_446|>' is not marked as EOG\n",
            "load: control token:    305 '<|reserved_295|>' is not marked as EOG\n",
            "load: control token:    160 '<|reserved_150|>' is not marked as EOG\n",
            "load: control token:    378 '<|reserved_368|>' is not marked as EOG\n",
            "load: control token:    404 '<|reserved_394|>' is not marked as EOG\n",
            "load: control token:    232 '<|reserved_222|>' is not marked as EOG\n",
            "load: control token:    348 '<|reserved_338|>' is not marked as EOG\n",
            "load: control token:    419 '<|reserved_409|>' is not marked as EOG\n",
            "load: control token:    287 '<|reserved_277|>' is not marked as EOG\n",
            "load: control token:     86 '<|reserved_76|>' is not marked as EOG\n",
            "load: control token:     58 '<|reserved_48|>' is not marked as EOG\n",
            "load: control token:    183 '<|reserved_173|>' is not marked as EOG\n",
            "load: control token:    369 '<|reserved_359|>' is not marked as EOG\n",
            "load: control token:    210 '<|reserved_200|>' is not marked as EOG\n",
            "load: control token:    434 '<|reserved_424|>' is not marked as EOG\n",
            "load: control token:    323 '<|reserved_313|>' is not marked as EOG\n",
            "load: control token:    268 '<|reserved_258|>' is not marked as EOG\n",
            "load: control token:    197 '<|reserved_187|>' is not marked as EOG\n",
            "load: control token:    184 '<|reserved_174|>' is not marked as EOG\n",
            "load: control token:    325 '<|reserved_315|>' is not marked as EOG\n",
            "load: control token:    138 '<|reserved_128|>' is not marked as EOG\n",
            "load: control token:    473 '<|reserved_463|>' is not marked as EOG\n",
            "load: control token:    150 '<|reserved_140|>' is not marked as EOG\n",
            "load: control token:    176 '<|reserved_166|>' is not marked as EOG\n",
            "load: control token:    280 '<|reserved_270|>' is not marked as EOG\n",
            "load: control token:    294 '<|reserved_284|>' is not marked as EOG\n",
            "load: control token:    194 '<|reserved_184|>' is not marked as EOG\n",
            "load: control token:    163 '<|reserved_153|>' is not marked as EOG\n",
            "load: control token:     88 '<|reserved_78|>' is not marked as EOG\n",
            "load: control token:    104 '<|reserved_94|>' is not marked as EOG\n",
            "load: control token:    139 '<|reserved_129|>' is not marked as EOG\n",
            "load: control token:    211 '<|reserved_201|>' is not marked as EOG\n",
            "load: control token:    129 '<|reserved_119|>' is not marked as EOG\n",
            "load: control token:    495 '<|reserved_485|>' is not marked as EOG\n",
            "load: control token:    213 '<|reserved_203|>' is not marked as EOG\n",
            "load: control token:    251 '<|reserved_241|>' is not marked as EOG\n",
            "load: control token:      1 '<|startoftext|>' is not marked as EOG\n",
            "load: control token:    478 '<|reserved_468|>' is not marked as EOG\n",
            "load: control token:    304 '<|reserved_294|>' is not marked as EOG\n",
            "load: control token:    391 '<|reserved_381|>' is not marked as EOG\n",
            "load: control token:    421 '<|reserved_411|>' is not marked as EOG\n",
            "load: control token:    490 '<|reserved_480|>' is not marked as EOG\n",
            "load: control token:    352 '<|reserved_342|>' is not marked as EOG\n",
            "load: control token:    386 '<|reserved_376|>' is not marked as EOG\n",
            "load: control token:    458 '<|reserved_448|>' is not marked as EOG\n",
            "load: control token:    308 '<|reserved_298|>' is not marked as EOG\n",
            "load: control token:    267 '<|reserved_257|>' is not marked as EOG\n",
            "load: control token:     97 '<|reserved_87|>' is not marked as EOG\n",
            "load: control token:     76 '<|reserved_66|>' is not marked as EOG\n",
            "load: control token:    447 '<|reserved_437|>' is not marked as EOG\n",
            "load: control token:    117 '<|reserved_107|>' is not marked as EOG\n",
            "load: control token:    191 '<|reserved_181|>' is not marked as EOG\n",
            "load: control token:     75 '<|reserved_65|>' is not marked as EOG\n",
            "load: control token:    413 '<|reserved_403|>' is not marked as EOG\n",
            "load: control token:     20 '<|reserved_10|>' is not marked as EOG\n",
            "load: control token:    366 '<|reserved_356|>' is not marked as EOG\n",
            "load: control token:    247 '<|reserved_237|>' is not marked as EOG\n",
            "load: control token:     89 '<|reserved_79|>' is not marked as EOG\n",
            "load: control token:    333 '<|reserved_323|>' is not marked as EOG\n",
            "load: control token:    174 '<|reserved_164|>' is not marked as EOG\n",
            "load: control token:    212 '<|reserved_202|>' is not marked as EOG\n",
            "load: control token:      4 '<|fim_mid|>' is not marked as EOG\n",
            "load: control token:    324 '<|reserved_314|>' is not marked as EOG\n",
            "load: control token:    334 '<|reserved_324|>' is not marked as EOG\n",
            "load: control token:    208 '<|reserved_198|>' is not marked as EOG\n",
            "load: control token:    168 '<|reserved_158|>' is not marked as EOG\n",
            "load: control token:     16 '<|reserved_6|>' is not marked as EOG\n",
            "load: control token:    329 '<|reserved_319|>' is not marked as EOG\n",
            "load: control token:    364 '<|reserved_354|>' is not marked as EOG\n",
            "load: control token:     62 '<|reserved_52|>' is not marked as EOG\n",
            "load: control token:    338 '<|reserved_328|>' is not marked as EOG\n",
            "load: control token:    110 '<|reserved_100|>' is not marked as EOG\n",
            "load: control token:    433 '<|reserved_423|>' is not marked as EOG\n",
            "load: control token:     32 '<|reserved_22|>' is not marked as EOG\n",
            "load: control token:    452 '<|reserved_442|>' is not marked as EOG\n",
            "load: control token:    148 '<|reserved_138|>' is not marked as EOG\n",
            "load: control token:    331 '<|reserved_321|>' is not marked as EOG\n",
            "load: control token:    469 '<|reserved_459|>' is not marked as EOG\n",
            "load: control token:    375 '<|reserved_365|>' is not marked as EOG\n",
            "load: control token:    252 '<|reserved_242|>' is not marked as EOG\n",
            "load: control token:     67 '<|reserved_57|>' is not marked as EOG\n",
            "load: control token:    389 '<|reserved_379|>' is not marked as EOG\n",
            "load: control token:     93 '<|reserved_83|>' is not marked as EOG\n",
            "load: control token:     40 '<|reserved_30|>' is not marked as EOG\n",
            "load: control token:    479 '<|reserved_469|>' is not marked as EOG\n",
            "load: control token:    395 '<|reserved_385|>' is not marked as EOG\n",
            "load: control token:      0 '<|pad|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('<|endoftext|>')\n",
            "load:   - 7 ('<|im_end|>')\n",
            "load: special tokens cache size = 507\n",
            "load: token to piece cache size = 0.3756 MB\n",
            "print_info: arch             = lfm2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 128000\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 16\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = [0, 0, 8, 0, 0, 8, 0, 0, 8, 0, 8, 0, 8, 0, 8, 0]\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = [0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n",
            "print_info: n_embd_k_gqa     = [0, 0, 512, 0, 0, 512, 0, 0, 512, 0, 512, 0, 512, 0, 512, 0]\n",
            "print_info: n_embd_v_gqa     = [0, 0, 512, 0, 0, 512, 0, 0, 512, 0, 512, 0, 512, 0, 512, 0]\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 128000\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.2B\n",
            "print_info: model params     = 1.17 B\n",
            "print_info: general.name     = LFM2 1.2B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 65536\n",
            "print_info: n_merges         = 63683\n",
            "print_info: BOS token        = 1 '<|startoftext|>'\n",
            "print_info: EOS token        = 7 '<|im_end|>'\n",
            "print_info: EOT token        = 2 '<|endoftext|>'\n",
            "print_info: PAD token        = 0 '<|pad|>'\n",
            "print_info: LF token         = 708 'ÄŠ'\n",
            "print_info: EOG token        = 2 '<|endoftext|>'\n",
            "print_info: EOG token        = 7 '<|im_end|>'\n",
            "print_info: max token length = 30\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 55 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   555.75 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   652.25 MiB\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.shortconv.out_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.shortconv.out_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.shortconv.out_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.shortconv.out_proj.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.shortconv.in_proj.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.shortconv.out_proj.weight with q4_0_8x8\n",
            "...\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (128000) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.25 MiB\n",
            "llama_kv_cache_unified: the V embeddings have different sizes across layers and FA is not enabled - padding V cache to 512\n",
            "llama_kv_cache_unified: layer   0: skipped\n",
            "llama_kv_cache_unified: layer   1: skipped\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: skipped\n",
            "llama_kv_cache_unified: layer   4: skipped\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: skipped\n",
            "llama_kv_cache_unified: layer   7: skipped\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: skipped\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: skipped\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: skipped\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: skipped\n",
            "llama_kv_cache_unified:        CPU KV buffer size =     6.00 MiB\n",
            "llama_kv_cache_unified: size =    6.00 MiB (   512 cells,   6 layers,  1/1 seqs), K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
            "llama_memory_recurrent, layer   0: dev = CPU\n",
            "llama_memory_recurrent, layer   1: dev = CPU\n",
            "llama_memory_recurrent: layer   2: skipped\n",
            "llama_memory_recurrent, layer   3: dev = CPU\n",
            "llama_memory_recurrent, layer   4: dev = CPU\n",
            "llama_memory_recurrent: layer   5: skipped\n",
            "llama_memory_recurrent, layer   6: dev = CPU\n",
            "llama_memory_recurrent, layer   7: dev = CPU\n",
            "llama_memory_recurrent: layer   8: skipped\n",
            "llama_memory_recurrent, layer   9: dev = CPU\n",
            "llama_memory_recurrent: layer  10: skipped\n",
            "llama_memory_recurrent, layer  11: dev = CPU\n",
            "llama_memory_recurrent: layer  12: skipped\n",
            "llama_memory_recurrent, layer  13: dev = CPU\n",
            "llama_memory_recurrent: layer  14: skipped\n",
            "llama_memory_recurrent, layer  15: dev = CPU\n",
            "llama_memory_recurrent:        CPU RS buffer size =     0.16 MiB\n",
            "llama_memory_recurrent: size =    0.16 MiB (     1 cells,  16 layers,  1 seqs), R (f32):    0.16 MiB, S (f32):    0.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 1184\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   139.03 MiB\n",
            "llama_context: graph nodes  = 572\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '2', 'tokenizer.ggml.add_sep_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '7', 'tokenizer.ggml.bos_token_id': '1', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_eos_token': 'false', 'lfm2.attention.layer_norm_rms_epsilon': '0.000010', 'lfm2.vocab_size': '65536', 'lfm2.rope.freq_base': '1000000.000000', 'general.architecture': 'lfm2', 'lfm2.context_length': '128000', 'tokenizer.ggml.padding_token_id': '0', 'general.basename': 'LFM2', 'lfm2.block_count': '16', 'tokenizer.ggml.pre': 'lfm2', 'general.name': 'LFM2 1.2B', 'general.type': 'model', 'general.size_label': '1.2B', 'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'general.license.link': 'LICENSE', 'lfm2.attention.head_count': '32', 'general.license': 'other', 'lfm2.embedding_length': '2048', 'lfm2.shortconv.l_cache': '3', 'general.license.name': 'lfm1.0', 'lfm2.feed_forward_length': '8192'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|startoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://970492998f2d.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [153]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     50.46.236.28:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     50.46.236.28:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    1507.51 ms\n",
            "llama_perf_context_print: prompt eval time =    1507.24 ms /    14 tokens (  107.66 ms per token,     9.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     639.64 ms /     6 runs   (  106.61 ms per token,     9.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2152.84 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     50.46.236.28:0 - \"GET /ask?userprompt=what%20is%20five%20minus%20one HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [153]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n@app.get(\"/predict\")\\ndef predict(prompt: str = Query(..., description=\"User input prompt\")):\\n    try:\\n        # Use existing dataset and trained model\\n        df = getDataFrame()\\n        all_data = getPrice(df).values\\n\\n        context_data = all_data[:-CONFIG[\\'days_to_predict\\']].reshape(-1, 1)\\n        real_future_data = all_data[-CONFIG[\\'days_to_predict\\']:].reshape(-1, 1)\\n\\n        predictions = predict_single_day(MODEL, context_data, real_future_data)\\n\\n        return {\\n            \"prompt\": prompt,\\n            \"predictions\": predictions.tolist(),\\n            \"real_data\": real_future_data.flatten().tolist()\\n        }\\n\\n    except Exception as e:\\n        return {\"error\": str(e)}\\n\\nif __name__ == \"__main__\":\\n    # Run FastAPI with uvicorn programmatically\\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "auth_token = \"31d1tKuvbHOakty4B7VyUzO5JSO_4gUsWjk8ZgCYVPh2d8WRA\" #for pyngrok to expose the loopback webpage locally\n",
        "\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "app = FastAPI(title=\"TimesFM\")\n",
        "\n",
        "console = Console()\n",
        "\n",
        "\n",
        "\n",
        "# Model file path. You can download the GGUF model files from https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF\n",
        "### CHANGE THIS TO THE MODEL YOU WANT TO USE ###\n",
        "MODEL_GGUF_FILE = \"./LFM2-1.2B-Q4_0.gguf\"\n",
        "\n",
        "llm = Llama(\n",
        "        model_path=MODEL_GGUF_FILE,\n",
        "    )\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def index():\n",
        "  return {\"message\": \"HI\"}\n",
        "\n",
        "@app.get(\"/ask\")\n",
        "async def ask(userprompt: str):\n",
        "  llm.reset()\n",
        "  history = [{\"role\": \"user\", \"content\": userprompt}]\n",
        "  output = llm.create_chat_completion(messages=history)\n",
        "  generated_message = output[\"choices\"][0][\"message\"][\"content\"]\n",
        "  return {\"userprompt\": userprompt, \"llm\": generated_message}\n",
        "\n",
        "\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "uvicorn.run(app, port=8000)\n",
        "'''\n",
        "\n",
        "@app.get(\"/predict\")\n",
        "def predict(prompt: str = Query(..., description=\"User input prompt\")):\n",
        "    try:\n",
        "        # Use existing dataset and trained model\n",
        "        df = getDataFrame()\n",
        "        all_data = getPrice(df).values\n",
        "\n",
        "        context_data = all_data[:-CONFIG['days_to_predict']].reshape(-1, 1)\n",
        "        real_future_data = all_data[-CONFIG['days_to_predict']:].reshape(-1, 1)\n",
        "\n",
        "        predictions = predict_single_day(MODEL, context_data, real_future_data)\n",
        "\n",
        "        return {\n",
        "            \"prompt\": prompt,\n",
        "            \"predictions\": predictions.tolist(),\n",
        "            \"real_data\": real_future_data.flatten().tolist()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run FastAPI with uvicorn programmatically\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}